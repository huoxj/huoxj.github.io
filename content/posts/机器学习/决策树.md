---
title: "决策树"
date: 2024-09-30T08:54:34-08:00
categories: 
- "机器学习"
featureimage: https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301003448.png
summary: "决策树就是一颗 `if-else` 树。树的每一个结点代表一个决策（测试），同时也代表了这个决策所对应的一个样本空间。我们的目标就是通过多次产生 `if-else` 分支，尽可能地让决策树的叶子结点只..."
---

## 基本思想

决策树就是一颗 `if-else` 树。树的每一个结点代表一个决策（测试），同时也代表了这个决策所对应的一个样本空间。我们的目标就是通过多次产生 `if-else` 分支，尽可能地让决策树的叶子结点只包含相同标签的样本。

既然是一棵树，我们就从**递归建树**的角度来理解决策树的基本思想。

### 基础

决策树的根节点代表了整个样本空间，没有任何划分或决策。

下图的例子只有连续属性，将就看吧。

![](https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301003448.png)

### 递归

假设我们在某一结点下。只考虑这个结点所代表的子样本空间。

建树就是要加深树的深度，即产生 `if-else` 分支。具体来说就是在当前的样本空间中插入决策边界，划分出的多个新的样本空间就是儿子们的样本空间。

我们每次递归只选择样本的**某一个属性**进行划分。

#### 离散属性

将这个属性的每一个取值都划出一个空间。

假如这个属性$a$取值集合是$D={x, y, z}$，我们就划出三个空间，对应这三个取值。这样这个结点下就生出了三棵子树。`if-else`如下：

```c
if(a == x) {
	SubTree1;
}else if(a == y) {
	SubTree2;
}else {
	SubTree3;
}
```

#### 连续属性

基本方法就是将连续属性变成离散属性。常用的方法是二分法。

对连续属性$A$，取合适的值$x$。通过如下函数，我们能得到一个离散属性$B=\{X, Y\}$。

$$
b=f(a)=\left\{
  \begin{aligned}
  X, a\le x \\
  Y, a\gt x
  \end{aligned}
  \right.
$$

取合适的值一般会取中位数。

![](https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301008219.png)

如上图，我们横着插了一条决策边界，这条边界代表如下决策：

```c
if(x <= 0.0596) {
	LeftSubTree;
}
else {
	RightSubTree;
}
```

### 停止条件

- 只要当前结点的样本空间只存在一类标签的样本，我们就停止递归此结点。

> 这样会导致十分严重的过拟合。解决方法看后面。

- 当前属性集为空, 或是所有样本在所有属性上取值相同，无法划分
- 当前结点包含的样本集合为空，不能划分

## 取最优划分属性

我能想到三种取决策边界的方法。

### 随便取

没错，~~随机永远是你大爷~~。我们随机取一个属性进行划分。

只要递归次数够多，一定能到达停止条件。只是这样建出来的树可能够让大半个中国在树底下乘凉了。

### 信息增益

> ID3 决策树

假定当前样本集合$D$中第$k$类样本所占的比例为$p_k$
定义**信息熵**如下：

$$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$

> 信息熵是度量样本集合“纯度”最常用的一种指标

说人话，**样本集合越杂，信息熵越大**。

假设离散属性$a$的取值集合为$\{a^1,a^2,...,a^V\}$，$D^v$是$D$中在$a$上取值等于$a^v$的样本集合。
由此定义以属性$a$对$D$进行划分的**信息增益**为：

$$
	Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
$$

可以看到，信息增益就是以属性$a$划分前后的信息熵差值来定义的。而划分后的信息熵是按样本数为权重求和来的。

信息增益越大，说明以这个属性划分后信息熵减小得越多，划分后的子样本空间最纯。所以我们对所有属性中**增益最大**的进行划分就好了。

### 增益率

> C4.5 决策树

只有一页 PPT。

增益率是对信息增益的补充。

![](https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301449718.png)

### 基尼指数

> CART 决策树

基尼指数**本质**反映了从$D$中随机抽取两个样例，其类别标记不一致的概率。

定义如下：

$$
\begin{equation}
	\begin{aligned}
		Gini(D) &= \sum^{|y|}_{k=1}\sum_{k^{'}\neq k}p_kp_{k^{'}} \\
				&= 1-\sum^{|y|}_{k=1}p_k^2 \\
				&= 1-\sum^{|y|}_{k=1}(\frac{|D^k|}{|D|})^2
	\end{aligned}
  \end{equation}
$$

其中，$p_k$是样本点属于第$k$类的概率，$D^k$是样本集合$D$中属于第$k$类的样本子集。

而属性$a$划分下的基尼指数为：

$$
	Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$

即按样本数对基尼指数加权平均求和。同样地，我们取$Gini\_index$最小那个属性就行了。

## 剪枝

前面说了，决策树的停止条件之一是叶子只包含一类标签的样本，而这样会导致很强的过拟合，在训练集上也会达到 100%的正确率。所以我们需要剪枝来增强泛化性能。

### 预剪枝

> 及早停止树的生长

#### 限制树的高度

这个很好理解。结点一旦长到一定高度了，就直接不让它继续往下长了。

#### 评估法

我随便起的名字。

思路是每次生长的时候，评估生长后的决策树在**验证集**上的表现。然后根据一定的策略决定是否继续生长下去。

比如，如果生长后验证集准确率降低了，我就不让树继续长了；否则，树就可以继续长。

#### 其他

剪枝这个东西，可操作空间相当大。根据实际任务去选择合适的剪枝方案才是可行之法。同时，不同的剪枝方法也可以相互融合，生成效果更好的剪枝。

### 后剪枝

> 随后删除或折叠信息量很少的结点

#### 评估法

还是评估法，没想到吧！

思路是将已经长好的结点收缩。

详细一点：

1. 我们考虑要剪枝的结点$k$，以$k$为根节点的子树有叶子$\{l_1, l_2, l_3...l_n\}$

2. 从 1 到 n，将这棵子树坍缩成叶子，并根据验证集评估整颗决策树。

3. 选择评估结果最好的叶子，再决定是否剪枝即可。

中间涉及的一些策略很含糊，因为这是根据实际去替换的部分。
