<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Runz&#39;s Blog</title>
    <link>https://huoxj.github.io/series/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Runz&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>cn</language>
    <lastBuildDate>Mon, 06 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://huoxj.github.io/series/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SE-ML-11 高斯混合模型</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml-11-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml-11-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/151671154&#34;&gt;如何通俗的理解高斯混合模型（Gaussian Mixture Models） - 知乎&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML02-模型评估与选择</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml02-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml02-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</guid>
      <description>&lt;h2 id=&#34;统计学习方法三要素&#34;&gt;统计学习方法三要素&lt;/h2&gt;&#xA;&lt;h3 id=&#34;模型&#34;&gt;模型&lt;/h3&gt;&#xA;&lt;p&gt;在假设空间中的一个具体假设。是一个需要学习的函数。&lt;/p&gt;&#xA;&lt;h3 id=&#34;策略&#34;&gt;策略&lt;/h3&gt;&#xA;&lt;p&gt;如何从假设空间中选择&lt;strong&gt;最优&lt;/strong&gt;的假设/模型。&lt;/p&gt;&#xA;&lt;p&gt;怎么判断模型优不优秀？&lt;strong&gt;损失函数&lt;/strong&gt;与风险函数。&lt;/p&gt;&#xA;&lt;h4 id=&#34;损失函数&#34;&gt;损失函数&lt;/h4&gt;&#xA;&lt;p&gt;评估 $f$ 在样本 $X$ 上，与真实值 $Y$ 的差距。&lt;/p&gt;&#xA;&lt;p&gt;常见的损失函数：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;平方损失函数 $$L(Y,f(X))=(Y-f(X)^2)$$&lt;/li&gt;&#xA;&lt;li&gt;0-1损失函数 $$L(Y,f(X))=\begin{equation}\begin{cases}1,Y\neq f(X) \ \0,Y= f(X)\end{cases} \end{equation} $$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;风险函数&#34;&gt;风险函数&lt;/h4&gt;&#xA;&lt;p&gt;经验风险：模型在训练数据集上的平均损失&#xA;结构风险：在经验风险的基础上，增加对模型复杂度，或者其他自定义惩罚&lt;/p&gt;&#xA;&lt;p&gt;两种风险越低，模型越优。&lt;/p&gt;&#xA;&lt;h3 id=&#34;算法&#34;&gt;算法&lt;/h3&gt;&#xA;&lt;p&gt;学习模型的具体方法。即损失函数或者风险函数的最优化问题。&lt;/p&gt;&#xA;&lt;p&gt;有两种情况：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;问题存在解析解，直接求解问题就可以得到最优模型&lt;/li&gt;&#xA;&lt;li&gt;问题没有解析解，那就需要梯度下降、牛顿法这种方式去逼近最优解&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;最小二乘法就是存在解析解的问题，可以直接通过解方程求得使模型最优的参数。神经网络就需要不断通过梯度下降的方式更新权重。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;评估方法&#34;&gt;评估方法&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;解决 “如何获得测试结果” 的问题&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;测试集与训练集的划分&#34;&gt;测试集与训练集的划分&lt;/h3&gt;&#xA;&lt;h4 id=&#34;留出法&#34;&gt;留出法&lt;/h4&gt;&#xA;&lt;p&gt;将拥有的数据集，一部分划作训练集，剩下就是测试集。&lt;/p&gt;&#xA;&lt;p&gt;注意：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;保持数据分布一致性（分层采样）&lt;/li&gt;&#xA;&lt;li&gt;多次重复划分（？没看懂）&lt;/li&gt;&#xA;&lt;li&gt;测试集的比例，在 20% ~ 33% 间&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;k-折交叉验证法&#34;&gt;K-折交叉验证法&lt;/h4&gt;&#xA;&lt;p&gt;将测试集分为 K 份。进行 k 次训练。每一次按顺序选取第 i 份作为测试集，其他作为训练集。将 k 次的测试结果取平均，作为最终的结果。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202501050857236.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;自助法&#34;&gt;自助法&lt;/h4&gt;&#xA;&lt;p&gt;一种可重复采样的方法。训练集和测试集可能有交集。&lt;/p&gt;&#xA;&lt;h3 id=&#34;调参&#34;&gt;调参&lt;/h3&gt;&#xA;&lt;p&gt;在整个训练过程中，有两种参数：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;算法的参数。就是超参数，使用&lt;strong&gt;验证集&lt;/strong&gt;人工调参&lt;/li&gt;&#xA;&lt;li&gt;模型的参数。由学习确定&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;在模型训练中，将验证集和训练集一起作为训练集。&lt;/p&gt;&#xA;&lt;h2 id=&#34;性能度量&#34;&gt;性能度量&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;衡量模型泛化能力&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML03-线性模型</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml03-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml03-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;h2 id=&#34;线性可分性&#34;&gt;线性可分性&lt;/h2&gt;&#xA;&lt;p&gt;任意两个向量，分别属于两个类别。这两个向量和某个确定的权重向量的线性组合得到的标量值，如果一定落在某个标量值的两侧，那么这两个类别是线性可分的。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202501050935696.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;感知机&#34;&gt;感知机&lt;/h2&gt;&#xA;&lt;p&gt;经典的二分类、线性、判别模型。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;f(x)=sign(\textbf{w}^T\textbf{x}+b)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;$\textbf{w}$ 是平面的法向量，$b$ 是到原点位移。用这个超平面来判别。&lt;/p&gt;&#xA;&lt;h2 id=&#34;线性回归&#34;&gt;线性回归&lt;/h2&gt;&#xA;&lt;h3 id=&#34;一元线性回归&#34;&gt;一元线性回归&lt;/h3&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;高中数学知识&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;最小化均方误差。进行一堆最小二乘估计，得到最优解：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;w=\frac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x^2_i-\frac{1}{m}(\sum_{i=1}^mx_i)^2}&#xA;\quad\quad&#xA;b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)&#xA;$$&lt;/p&gt;&#xA;&lt;h3 id=&#34;多元线性回归&#34;&gt;多元线性回归&lt;/h3&gt;&#xA;&lt;p&gt;$$&#xA;f(x)=\textbf{w}^T\textbf{x}+b \quad 使得\quad f(\textbf{x}_i\simeq y_i)&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;求解思想是类似的，但是涉及矩阵求逆。&lt;/p&gt;&#xA;&lt;h2 id=&#34;广义线性模型&#34;&gt;广义线性模型&lt;/h2&gt;&#xA;&lt;p&gt;给线性组合加上一个&lt;strong&gt;单调可微&lt;/strong&gt;的联系函数，这个联系函数可以是非线性的。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;y=g^{-1}(\textbf{w}^t\textbf{x}+b)&#xA;$$&lt;/p&gt;&#xA;&lt;h2 id=&#34;二分类任务&#34;&gt;二分类任务&lt;/h2&gt;&#xA;&lt;p&gt;使用特殊的联系函数，使得输出满足 $y\in{0,1}$&lt;/p&gt;&#xA;&lt;h3 id=&#34;联系函数&#34;&gt;联系函数&lt;/h3&gt;&#xA;&lt;h4 id=&#34;单位阶跃函数&#34;&gt;单位阶跃函数&lt;/h4&gt;&#xA;&lt;p&gt;$$&#xA;y=\begin{equation}\begin{cases}0,&amp;amp;z&amp;lt;0\0.5,&amp;amp;z=0\1,&amp;amp;z&amp;gt;0\end{cases}\end{equation}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;性质不好，因为不是连续函数。不可微。&lt;/p&gt;&#xA;&lt;h4 id=&#34;对数几率函数&#34;&gt;对数几率函数&lt;/h4&gt;&#xA;&lt;p&gt;大名鼎鼎 Logistic function。高中生物里种群数量的 S 形曲线。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;y=\frac{1}{1+e^-z}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;衍生出了 Logistic 回归。&lt;/p&gt;&#xA;&lt;h3 id=&#34;logistic-回归&#34;&gt;Logistic 回归&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;无需假设数据分布&lt;/li&gt;&#xA;&lt;li&gt;可以得到类别的近似概率预测&lt;/li&gt;&#xA;&lt;li&gt;使用数值优化算法求最优解&lt;/li&gt;&#xA;&lt;li&gt;是&lt;strong&gt;分类算法&lt;/strong&gt;，而不是回归算法&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;以 Logistic 函数作为联系函数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202501051007913.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;最后的 $ln\frac{y}{1-y}$ 称为“对数几率”。反映了 x 作为正例的相对可能性。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML06-支持向量机</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml06-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml06-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</guid>
      <description>&lt;h2 id=&#34;线性-svm&#34;&gt;线性 SVM&lt;/h2&gt;&#xA;&lt;p&gt;线性 SVM &lt;strong&gt;最大化&lt;/strong&gt;所有训练样本到决策平面的&lt;strong&gt;最小距离&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;具有最小距离的样本，叫支持向量。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202501051110666.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;计算-margin&#34;&gt;计算 margin&lt;/h3&gt;&#xA;&lt;p&gt;给定模型 $f(\textbf{x})=\textbf{w}^T\textbf{x}+b$，和任一样本 $x$ ，求其到超平面的距离 $r$。&lt;/p&gt;&#xA;&lt;p&gt;$r=\frac{|f(x)|}{||w||}$&lt;/p&gt;&#xA;&lt;p&gt;需要优化的式子出来了：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\mathop{argmax}\limits_{\textbf{w},b}(\mathop{min}\limits_{i}(r_i))&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;可以看到，这个式子的取值完全由间隔最小的样本决定。即模型的参数完全是由&lt;strong&gt;支持向量&lt;/strong&gt;决定的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;分类&#34;&gt;分类&lt;/h3&gt;&#xA;&lt;p&gt;$f(x) &amp;gt; 0$ 分为正类，否则是反类。我们的目标就是让 SVM 尽量能完全分开两个类。&lt;/p&gt;&#xA;&lt;p&gt;至于如何判断预测是否正确，如果正类的真实值是 1，反类是 -1 的话&lt;/p&gt;&#xA;&lt;p&gt;$y_if(x_i)$ 为正值，预测正确。否则预测错误。&lt;/p&gt;&#xA;&lt;p&gt;利用这个性质，需要优化的式子可以这样变，以保证 SVM 的可分性：&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\mathop{argmax}\limits_{\textbf{w},b}(\mathop{min}\limits_{i}(\frac{y_if(x_i)}{\textbf{||w||}}))&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;既考虑了预测的正确性，也考虑了最大化间隔的目标。&lt;/p&gt;&#xA;&lt;h3 id=&#34;进一步优化&#34;&gt;进一步优化&lt;/h3&gt;&#xA;&lt;p&gt;要优化的式子还是不够简单。我们从优化结果反着考虑：&lt;/p&gt;&#xA;&lt;p&gt;假设我们最终得到的最优的模型的参数是 $(\textbf{w},b)$，考虑一个参数为 $(c\textbf{w},cb)$ 的模型，其中 $c&amp;gt;0$。&lt;/p&gt;&#xA;&lt;p&gt;可以发现，这个新模型和最优的模型一毛一样&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于样本 $x_i$，二者预测结果一致。因为 $c$ 根本不影响符号&lt;/li&gt;&#xA;&lt;li&gt;支持向量以及间隔不变，可以参考 $r$ 的公式，$c$ 都抵消掉了&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;所以，我们可以通过引入这个参数 $c$ ，使得要优化的式子更简单。&lt;/p&gt;&#xA;&lt;p&gt;通过 $c$ 改变 $||w||=1$ 是一个不错的思路，但细想还是会发现，仍然比较复杂。&lt;/p&gt;&#xA;&lt;p&gt;直接揭晓答案：通过 $c$，在约束 $\mathop{min}\limits_{i}(y_if(x_i))=1$，同时最大化 $\frac{1}{||\textbf{w}||}$。&lt;/p&gt;&#xA;&lt;h3 id=&#34;拉格朗日乘子法&#34;&gt;拉格朗日乘子法&lt;/h3&gt;&#xA;&lt;p&gt;在约束条件下优化，可以用拉格朗日乘子法解决。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML08-神经网络</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml08-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml08-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>&lt;h2 id=&#34;结构&#34;&gt;结构&lt;/h2&gt;&#xA;&lt;p&gt;输入 -&amp;gt; 权值 -&amp;gt; 激活函数 -&amp;gt; 偏置单元&lt;/p&gt;&#xA;&lt;h2 id=&#34;激活函数&#34;&gt;激活函数&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单位跃迁函数&lt;/li&gt;&#xA;&lt;li&gt;饱和激励函数：Sigmoid、tanh&lt;/li&gt;&#xA;&lt;li&gt;非饱和激励函数：ReLU 系列&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Pytorch DataLoader 加速</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch-dataloader-%E5%8A%A0%E9%80%9F/</link>
      <pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch-dataloader-%E5%8A%A0%E9%80%9F/</guid>
      <description>&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;&#xA;&lt;p&gt;在训练机器学习作业的 ResNet 网络时，发现每一个 epoch 之前都会卡上十几二十秒才开始 模型真正的训练。具体表现如下：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;每个 epoch 的 tqdm 进度条之间要卡十几秒才会出现&#xA;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202412022032540.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;li&gt;显卡占用率时高时低，没有全力干活&#xA;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202412022042166.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;原因&#34;&gt;原因&lt;/h2&gt;&#xA;&lt;p&gt;一句话概括，是每个 epoch 前&lt;strong&gt;数据读取到内存再到显存&lt;/strong&gt;占用了绝大部分时间。&lt;/p&gt;&#xA;&lt;p&gt;看下面两张图，时间都花到读数据了，显卡基本全程摸鱼。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202412022151904.png&#34; alt=&#34;&#34;&gt;&#xA;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202412022151536.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;详细来说，在每个 epoch 开始之前，dataloader 从传入的 dataset 中通过 getitem，将初始化时准备好的数据返回。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#8fbcbb&#34;&gt;MyDataset&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;Dataset&lt;span style=&#34;color:#eceff4&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;&lt;span style=&#34;color:#81a1c1&#34;&gt;...&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;def&lt;/span&gt; __getitem__&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;self&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; idx&lt;span style=&#34;color:#eceff4&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;&#x9;image &lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#81a1c1&#34;&gt;.&lt;/span&gt;images&lt;span style=&#34;color:#eceff4&#34;&gt;[&lt;/span&gt;idx&lt;span style=&#34;color:#eceff4&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        label &lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#81a1c1&#34;&gt;.&lt;/span&gt;labels&lt;span style=&#34;color:#eceff4&#34;&gt;[&lt;/span&gt;idx&lt;span style=&#34;color:#eceff4&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;if&lt;/span&gt; self&lt;span style=&#34;color:#81a1c1&#34;&gt;.&lt;/span&gt;transform&lt;span style=&#34;color:#eceff4&#34;&gt;:&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            image &lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#81a1c1&#34;&gt;.&lt;/span&gt;transform&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;image&lt;span style=&#34;color:#eceff4&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;return&lt;/span&gt; image&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; label&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;而且，这里 transform 将图像转换成 tensor 也花了不少时间。这种与数据增强无关的 transform 显然可以提前处理好。但这里每次读取都要 transform 一遍，很浪费。&lt;/p&gt;&#xA;&lt;p&gt;再来看 dataloader：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trainLoader &lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt; Data&lt;span style=&#34;color:#81a1c1&#34;&gt;.&lt;/span&gt;DataLoader&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;trainSet&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; batch_size&lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt;batch_size&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; shuffle&lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; num_workers&lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b48ead&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;,&lt;/span&gt; drop_last&lt;span style=&#34;color:#81a1c1&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;True&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;windows 上设置 num_workers 不为 0 时会让数据提供给 cpu，也就是到内存里去。这样我们 getitem 返回的数据又被拷贝了一份。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML05-K 邻近</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml05-k-%E9%82%BB%E8%BF%91/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml05-k-%E9%82%BB%E8%BF%91/</guid>
      <description>&lt;h2 id=&#34;k-nn-分类器&#34;&gt;k-NN 分类器&lt;/h2&gt;&#xA;&lt;h3 id=&#34;算法流程&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;p&gt;对测试样本，找训练样本中最近的 $k$ 个，这 $k$ 个样本中&lt;strong&gt;标签最多&lt;/strong&gt;的就是测试样本的类。&lt;/p&gt;&#xA;&lt;h3 id=&#34;k-的取值&#34;&gt;k 的取值&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;k 一般取奇数值，避免平局&lt;/li&gt;&#xA;&lt;li&gt;k 取不同的值，分类结果可能不同&lt;/li&gt;&#xA;&lt;li&gt;k 值较小时，对噪声敏感，整体模型变得复杂，容易过拟合&lt;/li&gt;&#xA;&lt;li&gt;k 值较大时，对噪声不敏感，整体模型变得简单，容易欠拟合&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;变种&#34;&gt;变种&lt;/h3&gt;&#xA;&lt;h4 id=&#34;最邻近分类器&#34;&gt;最邻近分类器&lt;/h4&gt;&#xA;&lt;p&gt;k-NN 的 $k=1$ 的特殊情况。&lt;/p&gt;&#xA;&lt;p&gt;泛化错误率，不超过贝叶斯分类器错误率的两倍。&lt;/p&gt;&#xA;&lt;h2 id=&#34;k-nn-回归&#34;&gt;k-NN 回归&lt;/h2&gt;&#xA;&lt;h3 id=&#34;算法流程-1&#34;&gt;算法流程&lt;/h3&gt;&#xA;&lt;p&gt;对测试样本，找训练样本中最近的 $k$ 个，将这 $k$ 个样本的标签&lt;strong&gt;加权平均&lt;/strong&gt;得到预测值。&lt;/p&gt;&#xA;&lt;h3 id=&#34;近邻平滑&#34;&gt;近邻平滑&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;二次核&lt;/li&gt;&#xA;&lt;li&gt;次方核&lt;/li&gt;&#xA;&lt;li&gt;高斯核&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;降低近邻计算&#34;&gt;降低近邻计算&lt;/h2&gt;&#xA;&lt;h3 id=&#34;维诺图&#34;&gt;维诺图&lt;/h3&gt;&#xA;&lt;p&gt;就是类别预测图，根据决策边界，将边界内填色。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202501051051813.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;kd-tree&#34;&gt;KD-Tree&lt;/h3&gt;&#xA;&lt;p&gt;OI-wiki 讲得更好一点。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://oi-wiki.org/ds/kdt/&#34;&gt;K-D Tree - OI Wiki&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;选择维度，标准是方差&lt;/li&gt;&#xA;&lt;li&gt;选择切割点，可以选第一步选择的维度中，为中位数的点&lt;/li&gt;&#xA;&lt;li&gt;切割点作为当前根节点，切割的两个空间作为左右子树&lt;/li&gt;&#xA;&lt;li&gt;递归。直到当前空间只有一个点&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>SE-ML04-决策树</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml04-%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Mon, 30 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml04-%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>&lt;h2 id=&#34;基本思想&#34;&gt;基本思想&lt;/h2&gt;&#xA;&lt;p&gt;决策树就是一颗 &lt;code&gt;if-else&lt;/code&gt; 树。树的每一个结点代表一个决策（测试），同时也代表了这个决策所对应的一个样本空间。我们的目标就是通过多次产生 &lt;code&gt;if-else&lt;/code&gt; 分支，尽可能地让决策树的叶子结点只包含相同标签的样本。&lt;/p&gt;&#xA;&lt;p&gt;既然是一棵树，我们就从&lt;strong&gt;递归建树&lt;/strong&gt;的角度来理解决策树的基本思想。&lt;/p&gt;&#xA;&lt;h3 id=&#34;基础&#34;&gt;基础&lt;/h3&gt;&#xA;&lt;p&gt;决策树的根节点代表了整个样本空间，没有任何划分或决策。&lt;/p&gt;&#xA;&lt;p&gt;下图的例子只有连续属性，将就看吧。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301003448.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;递归&#34;&gt;递归&lt;/h3&gt;&#xA;&lt;p&gt;假设我们在某一结点下。只考虑这个结点所代表的子样本空间。&lt;/p&gt;&#xA;&lt;p&gt;建树就是要加深树的深度，即产生 &lt;code&gt;if-else&lt;/code&gt; 分支。具体来说就是在当前的样本空间中插入决策边界，划分出的多个新的样本空间就是儿子们的样本空间。&lt;/p&gt;&#xA;&lt;p&gt;我们每次递归只选择样本的&lt;strong&gt;某一个属性&lt;/strong&gt;进行划分。&lt;/p&gt;&#xA;&lt;h4 id=&#34;离散属性&#34;&gt;离散属性&lt;/h4&gt;&#xA;&lt;p&gt;将这个属性的每一个取值都划出一个空间。&lt;/p&gt;&#xA;&lt;p&gt;假如这个属性$a$取值集合是$D={x, y, z}$，我们就划出三个空间，对应这三个取值。这样这个结点下就生出了三棵子树。&lt;code&gt;if-else&lt;/code&gt;如下：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;a &lt;span style=&#34;color:#81a1c1&#34;&gt;==&lt;/span&gt; x&lt;span style=&#34;color:#eceff4&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#eceff4&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;SubTree1&lt;span style=&#34;color:#eceff4&#34;&gt;;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;a &lt;span style=&#34;color:#81a1c1&#34;&gt;==&lt;/span&gt; y&lt;span style=&#34;color:#eceff4&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#eceff4&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;SubTree2&lt;span style=&#34;color:#eceff4&#34;&gt;;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#eceff4&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;SubTree3&lt;span style=&#34;color:#eceff4&#34;&gt;;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;连续属性&#34;&gt;连续属性&lt;/h4&gt;&#xA;&lt;p&gt;基本方法就是将连续属性变成离散属性。常用的方法是二分法。&lt;/p&gt;&#xA;&lt;p&gt;对连续属性$A$，取合适的值$x$。通过如下函数，我们能得到一个离散属性$B={X, Y}$。&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;b=f(a)=\left{&#xA;\begin{aligned}&#xA;X, a\le x \&#xA;Y, a\gt x&#xA;\end{aligned}&#xA;\right.&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;取合适的值一般会取中位数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409301008219.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;如上图，我们横着插了一条决策边界，这条边界代表如下决策：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#d8dee9;background-color:#2e3440;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;(&lt;/span&gt;x &lt;span style=&#34;color:#81a1c1&#34;&gt;&amp;lt;=&lt;/span&gt; &lt;span style=&#34;color:#b48ead&#34;&gt;0.0596&lt;/span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#eceff4&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;LeftSubTree&lt;span style=&#34;color:#eceff4&#34;&gt;;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#81a1c1;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#eceff4&#34;&gt;{&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;RightSubTree&lt;span style=&#34;color:#eceff4&#34;&gt;;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#eceff4&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;停止条件&#34;&gt;停止条件&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;只要当前结点的样本空间只存在一类标签的样本，我们就停止递归此结点。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;这样会导致十分严重的过拟合。解决方法看后面。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;当前属性集为空，或是所有样本在所有属性上取值相同，无法划分&lt;/li&gt;&#xA;&lt;li&gt;当前结点包含的样本集合为空，不能划分&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;取最优划分属性&#34;&gt;取最优划分属性&lt;/h2&gt;&#xA;&lt;p&gt;我能想到三种取决策边界的方法。&lt;/p&gt;&#xA;&lt;h3 id=&#34;随便取&#34;&gt;随便取&lt;/h3&gt;&#xA;&lt;p&gt;没错，&lt;del&gt;随机永远是你大爷&lt;/del&gt;。我们随机取一个属性进行划分。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SE-ML01-概要</title>
      <link>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml01-%E6%A6%82%E8%A6%81/</link>
      <pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://huoxj.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/se-ml01-%E6%A6%82%E8%A6%81/</guid>
      <description>&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;&#xA;&lt;h3 id=&#34;经典机器学习&#34;&gt;经典机器学习&lt;/h3&gt;&#xA;&lt;p&gt;学习是一个蕴含特定目的的知识获取过程。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;内部表现为新知识的不断建立与修正&lt;/li&gt;&#xA;&lt;li&gt;外部表现为性能改善&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;现代机器学习&#34;&gt;现代机器学习&lt;/h3&gt;&#xA;&lt;p&gt;任何通过&lt;strong&gt;数据训练&lt;/strong&gt;的&lt;strong&gt;学习算法&lt;/strong&gt;都属于机器学习&lt;/p&gt;&#xA;&lt;h2 id=&#34;基本术语&#34;&gt;基本术语&lt;/h2&gt;&#xA;&lt;h3 id=&#34;归纳偏好&#34;&gt;归纳偏好&lt;/h3&gt;&#xA;&lt;p&gt;ML 算法在学习过程中对某种类型假设的偏好。这种偏好一定存在。&lt;/p&gt;&#xA;&lt;p&gt;偏好最好与问题本身匹配，不然要用&lt;strong&gt;奥卡姆剃刀&lt;/strong&gt;删掉。&lt;/p&gt;&#xA;&lt;h3 id=&#34;nfl-定理&#34;&gt;NFL 定理&lt;/h3&gt;&#xA;&lt;p&gt;No Free Lunch 定理。一个算法 A 如果在某些问题上比另一个算法 B 好，必然存在另一些问题，B 比 A 好。&lt;/p&gt;&#xA;&lt;h3 id=&#34;分类和回归区别&#34;&gt;分类和回归区别&lt;/h3&gt;&#xA;&lt;p&gt;分类的输出是&lt;strong&gt;离散&lt;/strong&gt;值，回归的输出是&lt;strong&gt;连续&lt;/strong&gt;值。&lt;/p&gt;&#xA;&lt;h2 id=&#34;评价指标&#34;&gt;评价指标&lt;/h2&gt;&#xA;&lt;h3 id=&#34;混淆矩阵精度矩阵&#34;&gt;混淆矩阵、精度矩阵&lt;/h3&gt;&#xA;&lt;p&gt;精度矩阵是二分类下的混淆矩阵&lt;/p&gt;&#xA;&lt;h3 id=&#34;精度矩阵下的名词&#34;&gt;精度矩阵下的名词&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;TRUE&lt;/code&gt;/&lt;code&gt;FALSE&lt;/code&gt;: 预测结果与实际相符/不符&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;POSITIVE&lt;/code&gt;/&lt;code&gt;NEGATIVE&lt;/code&gt;: 预测为真(阳性)/假(阴性)&lt;/p&gt;&#xA;&lt;h3 id=&#34;算术指标&#34;&gt;算术指标&lt;/h3&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202409271712083.png&#34; alt=&#34;image-20240816164523881&#34;&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Accuracy 精度&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;预测正确的比例&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Sensitivity 敏感率&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;同查全率&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Specificity 特异率&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;阴性里预测正确的比例&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Precision 查准率&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;预测为阳性中真的阳了的比例&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Recall 查全率&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;所有阳性中预测出来的比例&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;统计学基本概念&#34;&gt;统计学基本概念&lt;/h2&gt;&#xA;&lt;h3 id=&#34;距离度量函数&#34;&gt;距离度量函数&lt;/h3&gt;&#xA;&lt;p&gt;对于两个样本 $x_i,x_j\in R^d$&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;欧式距离                $d(x_i, x_j)= \sqrt{(x_i-x_j)^T(x_i-x_j)}=||x_i-x_j||_2$&lt;/li&gt;&#xA;&lt;li&gt;曼哈顿距离            $d(x_i, x_j)=||x_i-x_j||_1$&lt;/li&gt;&#xA;&lt;li&gt;切比雪夫距离        $d(x_i, x_j)=||x_i-x_j||_\infty$&lt;/li&gt;&#xA;&lt;li&gt;余弦距离               $d(x_i, x_j)=\frac{x_i^Tx_j}{||x_i||\ ||x_j||}$&lt;/li&gt;&#xA;&lt;li&gt;马式距离               $d(x_i, x_j)=\sqrt{(x_i-x_j)^TM(x_i-x_j)}$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://runzblog.oss-cn-hangzhou.aliyuncs.com/postimg/202410252021162.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
